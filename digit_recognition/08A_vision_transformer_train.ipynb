{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import ViTForImageClassification, ViTFeatureExtractor\nimport torch\nfrom torch import nn, optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:33.228765Z","iopub.execute_input":"2024-08-31T17:11:33.229312Z","iopub.status.idle":"2024-08-31T17:11:52.030767Z","shell.execute_reply.started":"2024-08-31T17:11:33.229276Z","shell.execute_reply":"2024-08-31T17:11:52.029688Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_df = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:52.032553Z","iopub.execute_input":"2024-08-31T17:11:52.033113Z","iopub.status.idle":"2024-08-31T17:11:57.568165Z","shell.execute_reply.started":"2024-08-31T17:11:52.033076Z","shell.execute_reply":"2024-08-31T17:11:57.567114Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"y = train_df['label'].values\nX = train_df.drop('label', axis=1).values","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:57.569277Z","iopub.execute_input":"2024-08-31T17:11:57.569583Z","iopub.status.idle":"2024-08-31T17:11:57.659091Z","shell.execute_reply.started":"2024-08-31T17:11:57.569551Z","shell.execute_reply":"2024-08-31T17:11:57.658227Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:57.661113Z","iopub.execute_input":"2024-08-31T17:11:57.661467Z","iopub.status.idle":"2024-08-31T17:11:58.080764Z","shell.execute_reply.started":"2024-08-31T17:11:57.661433Z","shell.execute_reply":"2024-08-31T17:11:58.079748Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(X_train, dtype=torch.float32) / 255.0\nX_val = torch.tensor(X_val, dtype=torch.float32) / 255.0\nX_test = torch.tensor(test_df.values, dtype=torch.float32) / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.082004Z","iopub.execute_input":"2024-08-31T17:11:58.082332Z","iopub.status.idle":"2024-08-31T17:11:58.289596Z","shell.execute_reply.started":"2024-08-31T17:11:58.082297Z","shell.execute_reply":"2024-08-31T17:11:58.288667Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.view(-1, 1, 28, 28)\nX_val = X_val.view(-1, 1, 28, 28)\nX_test = X_test.view(-1, 1, 28, 28)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.290947Z","iopub.execute_input":"2024-08-31T17:11:58.291375Z","iopub.status.idle":"2024-08-31T17:11:58.301623Z","shell.execute_reply.started":"2024-08-31T17:11:58.291331Z","shell.execute_reply":"2024-08-31T17:11:58.300581Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.repeat(1, 3, 1, 1)\nX_val = X_val.repeat(1, 3, 1, 1)\nX_test = X_test.repeat(1, 3, 1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.304710Z","iopub.execute_input":"2024-08-31T17:11:58.305025Z","iopub.status.idle":"2024-08-31T17:11:58.572072Z","shell.execute_reply.started":"2024-08-31T17:11:58.304993Z","shell.execute_reply":"2024-08-31T17:11:58.571208Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"y_train = torch.tensor(y_train, dtype=torch.long)\ny_val = torch.tensor(y_val, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.573272Z","iopub.execute_input":"2024-08-31T17:11:58.573646Z","iopub.status.idle":"2024-08-31T17:11:58.579220Z","shell.execute_reply.started":"2024-08-31T17:11:58.573605Z","shell.execute_reply":"2024-08-31T17:11:58.578184Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MNISTDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.580685Z","iopub.execute_input":"2024-08-31T17:11:58.581148Z","iopub.status.idle":"2024-08-31T17:11:58.588781Z","shell.execute_reply.started":"2024-08-31T17:11:58.581104Z","shell.execute_reply":"2024-08-31T17:11:58.587883Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel grayscale for ViT\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.593457Z","iopub.execute_input":"2024-08-31T17:11:58.593736Z","iopub.status.idle":"2024-08-31T17:11:58.600949Z","shell.execute_reply.started":"2024-08-31T17:11:58.593707Z","shell.execute_reply":"2024-08-31T17:11:58.600003Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = MNISTDataset(X_train, y_train, transform=transform)\nval_dataset = MNISTDataset(X_val, y_val, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.602253Z","iopub.execute_input":"2024-08-31T17:11:58.602553Z","iopub.status.idle":"2024-08-31T17:11:58.615872Z","shell.execute_reply.started":"2024-08-31T17:11:58.602511Z","shell.execute_reply":"2024-08-31T17:11:58.614951Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.617018Z","iopub.execute_input":"2024-08-31T17:11:58.617456Z","iopub.status.idle":"2024-08-31T17:11:58.626723Z","shell.execute_reply.started":"2024-08-31T17:11:58.617414Z","shell.execute_reply":"2024-08-31T17:11:58.625831Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", num_labels=10, ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:11:58.627761Z","iopub.execute_input":"2024-08-31T17:11:58.628055Z","iopub.status.idle":"2024-08-31T17:12:17.979363Z","shell.execute_reply.started":"2024-08-31T17:11:58.628022Z","shell.execute_reply":"2024-08-31T17:12:17.978574Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734dbbe55f1644418dbd8fdea50894b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ec8700aa1c411481f72c8a8c5478d3"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:12:17.980670Z","iopub.execute_input":"2024-08-31T17:12:17.981299Z","iopub.status.idle":"2024-08-31T17:12:18.453978Z","shell.execute_reply.started":"2024-08-31T17:12:17.981251Z","shell.execute_reply":"2024-08-31T17:12:18.453082Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e149dbe5cd429b8337154bc9791f4a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:12:18.455244Z","iopub.execute_input":"2024-08-31T17:12:18.455629Z","iopub.status.idle":"2024-08-31T17:12:18.769118Z","shell.execute_reply.started":"2024-08-31T17:12:18.455583Z","shell.execute_reply":"2024-08-31T17:12:18.768188Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTSdpaAttention(\n            (attention): ViTSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:12:18.770445Z","iopub.execute_input":"2024-08-31T17:12:18.771200Z","iopub.status.idle":"2024-08-31T17:12:18.777256Z","shell.execute_reply.started":"2024-08-31T17:12:18.771136Z","shell.execute_reply":"2024-08-31T17:12:18.776134Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for epoch in range(15):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images).logits  # Get logits from ViT\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-31T17:12:18.778363Z","iopub.execute_input":"2024-08-31T17:12:18.778620Z","iopub.status.idle":"2024-08-31T19:54:26.299224Z","shell.execute_reply.started":"2024-08-31T17:12:18.778592Z","shell.execute_reply":"2024-08-31T19:54:26.298140Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.28945373427655013\nEpoch 2, Loss: 0.14735054519382262\nEpoch 3, Loss: 0.12659360226953315\nEpoch 4, Loss: 0.11202816581974427\nEpoch 5, Loss: 0.10198495909216858\nEpoch 6, Loss: 0.10186538216613587\nEpoch 7, Loss: 0.09289925378082053\nEpoch 8, Loss: 0.08796772828059538\nEpoch 9, Loss: 0.08271318506183369\nEpoch 10, Loss: 0.08411434030603795\nEpoch 11, Loss: 0.07940674679531227\nEpoch 12, Loss: 0.077881838565781\nEpoch 13, Loss: 0.07677217571951804\nEpoch 14, Loss: 0.07130551450841484\nEpoch 15, Loss: 0.07133108696235077\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model after training is done\nmodel_save_path = \"fine_tuned_vit_mnist.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(\"Model saved successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-31T19:54:26.300840Z","iopub.execute_input":"2024-08-31T19:54:26.301538Z","iopub.status.idle":"2024-08-31T19:54:26.731112Z","shell.execute_reply.started":"2024-08-31T19:54:26.301489Z","shell.execute_reply":"2024-08-31T19:54:26.730200Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model saved successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-09-01T00:02:29.060993Z","iopub.execute_input":"2024-09-01T00:02:29.061878Z","iopub.status.idle":"2024-09-01T00:02:29.068417Z","shell.execute_reply.started":"2024-09-01T00:02:29.061836Z","shell.execute_reply":"2024-09-01T00:02:29.067320Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'fine_tuned_vit_mnist.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-01T00:02:57.156866Z","iopub.execute_input":"2024-09-01T00:02:57.157231Z","iopub.status.idle":"2024-09-01T00:02:57.165451Z","shell.execute_reply.started":"2024-09-01T00:02:57.157194Z","shell.execute_reply":"2024-09-01T00:02:57.164343Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine_tuned_vit_mnist.pth","text/html":"<a href='fine_tuned_vit_mnist.pth' target='_blank'>fine_tuned_vit_mnist.pth</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}