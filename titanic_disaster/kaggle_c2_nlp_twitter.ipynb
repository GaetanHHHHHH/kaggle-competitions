{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with Disaster Tweets\n",
        "\n",
        "*Predict which Tweets are about real disasters and which ones are not.*\n",
        "\n",
        "This competition has as goal to apply the lesson 3 of the FastAI deep learning tutorial's [notebook](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) to another dataset.\n",
        "\n",
        "Additional resource used: [link](https://www.kaggle.com/code/mohamedabdullah/disaster-tweets-solution)."
      ],
      "metadata": {
        "id": "Wng0OKRgNEJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "fFBwWZJjR2Q1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byhbngKpMFoe"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv .kaggle /root/"
      ],
      "metadata": {
        "id": "4wXUUi_yRJLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "mZrxb2seRRes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "metadata": {
        "id": "05dSXRUxQaSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nlp-getting-started.zip"
      ],
      "metadata": {
        "id": "4x8G1a4WQjh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ],
      "metadata": {
        "id": "lwh6HYu5BiJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and structure"
      ],
      "metadata": {
        "id": "AD6hk-WQMW51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OECI4VOlRr_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./train.csv')"
      ],
      "metadata": {
        "id": "VNp-18vhR-_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lKmqAsFFR__r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object')"
      ],
      "metadata": {
        "id": "NjKP0BaaSf5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum().plot(kind='bar')"
      ],
      "metadata": {
        "id": "zwPu2rcdKg_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "color = [sns.xkcd_rgb['medium blue'], sns.xkcd_rgb['pale red']]\n",
        "sns.countplot(x='target',data = df, palette = color)\n",
        "plt.gca().set_ylabel('Samples')"
      ],
      "metadata": {
        "id": "__r-Q91LLMqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of character, word,  and sentence frequency"
      ],
      "metadata": {
        "id": "bx7eARI-MZ9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# count number of characters in each tweet\n",
        "df['char_len'] = df.text.str.len()\n",
        "\n",
        "# count number of words in each tweet\n",
        "word_tokens = [len(word_tokenize(tweet)) for tweet in df.text]\n",
        "df['word_len'] = word_tokens\n",
        "\n",
        "# count number of sentence in each tweet\n",
        "sent_tokens = [len(sent_tokenize(tweet)) for tweet in df.text]\n",
        "df['sent_len'] = sent_tokens\n",
        "\n",
        "plot_cols = ['char_len','word_len','sent_len']\n",
        "plot_titles = ['Character Length','Word Length','Sentence Length']\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "for counter, i in enumerate([0,1,2]):\n",
        "    plt.subplot(1,3,counter+1)\n",
        "    sns.distplot(df[df.target == 1][plot_cols[i]], label='Disaster', color=color[1]).set_title(plot_titles[i])\n",
        "    sns.distplot(df[df.target == 0][plot_cols[i]], label='Non-Disaster', color=color[0])\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "JdmwWzAtNIfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate the Outliers\n",
        "\n",
        "df[df.sent_len > 8]\n",
        "df[df.word_len > 50]\n",
        "\n",
        "# => make sure to deal with the punctuation"
      ],
      "metadata": {
        "id": "PmNrkr09N0NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot most common stopwords"
      ],
      "metadata": {
        "id": "TZdYj734O3H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot most common stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# Get all the word tokens in dataframe for Disaster and Non-Disaster\n",
        "corpus0 = [] # Non-Disaster\n",
        "[corpus0.append(word.lower()) for tweet in df[df.target == 0].text for word in word_tokenize(tweet)]\n",
        "corpus1 = [] # Disaster\n",
        "[corpus1.append(word.lower()) for tweet in df[df.target == 1].text for word in word_tokenize(tweet)]\n",
        "\n",
        "# Function for counting top stopwords in a corpus\n",
        "def count_top_stopwords(corpus):\n",
        "    stopwords_freq = {}\n",
        "    for word in corpus:\n",
        "        if word in stop:\n",
        "            if word in stopwords_freq:\n",
        "                stopwords_freq[word] += 1\n",
        "            else:\n",
        "                stopwords_freq[word] = 1\n",
        "    topwords = sorted(stopwords_freq.items(), key=lambda item: item[1], reverse=True)[:10] # get the top 10 stopwords\n",
        "    x,y = zip(*topwords) # get key and values\n",
        "    return x,y\n",
        "\n",
        "x0,y0 = count_top_stopwords(corpus0)\n",
        "x1,y1 = count_top_stopwords(corpus1)\n",
        "\n",
        "# Plot bar plot of top stopwords for each class\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.bar(x0,y0, color=color[0])\n",
        "plt.title('Top Stopwords for Non-Disaster Tweets')\n",
        "plt.subplot(1,2,2)\n",
        "plt.bar(x1,y1, color=color[1])\n",
        "plt.title('Top Stopwords for  Disaster Tweets')"
      ],
      "metadata": {
        "id": "Vaf_-FnKOL-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot most common punctuation"
      ],
      "metadata": {
        "id": "gcgkPpw2O7VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot most common punctuations\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "# Get all the punctuations in dataframe for Disaster and Non-Disaster\n",
        "corpus0 = [] # Non-Disaster\n",
        "[corpus0.append(c) for tweet in df[df.target == 0].text for c in tweet]\n",
        "corpus0 = list(filter(lambda x: x in punctuation, corpus0)) # use filter to select only punctuations\n",
        "corpus1 = [] # Disaster\n",
        "[corpus1.append(c) for tweet in df[df.target == 1].text for c in tweet]\n",
        "corpus1 = list(filter(lambda x: x in punctuation, corpus1))\n",
        "\n",
        "from collections import Counter\n",
        "x0,y0 = zip(*Counter(corpus0).most_common())\n",
        "x1,y1 = zip(*Counter(corpus1).most_common())\n",
        "\n",
        "# Plot bar plot of top punctuations for each class\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.bar(x0,y0, color=color[0])\n",
        "plt.title('Top Punctuations for Non-Disaster Tweets')\n",
        "plt.subplot(1,2,2)\n",
        "plt.bar(x1,y1, color=color[1])\n",
        "plt.title('Top Punctuations for Disaster Tweets')"
      ],
      "metadata": {
        "id": "PlNTtomhO9sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot most common words"
      ],
      "metadata": {
        "id": "zsZM-9EKPUaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot most common words\n",
        "import re\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "stop = ENGLISH_STOP_WORDS.union(stop) # combine stop words from different sources\n",
        "\n",
        "# function for removing url from text\n",
        "def remove_url(txt):\n",
        "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
        "\n",
        "# Get all the word tokens in dataframe for Disaster and Non-Disaster\n",
        "# - remove url, tokenize tweet into words, lowercase words\n",
        "corpus0 = [] # Non-Disaster\n",
        "[corpus0.append(word.lower()) for tweet in df[df.target == 0].text for word in word_tokenize(remove_url(tweet))]\n",
        "corpus0 = list(filter(lambda x: x not in stop, corpus0)) # use filter to unselect stopwords\n",
        "\n",
        "corpus1 = [] # Disaster\n",
        "[corpus1.append(word.lower()) for tweet in df[df.target == 1].text for word in word_tokenize(remove_url(tweet))]\n",
        "corpus1 = list(filter(lambda x: x not in stop, corpus1)) # use filter to unselect stopwords\n",
        "\n",
        "# Create df for word counts to use sns plots\n",
        "a = Counter(corpus0).most_common()\n",
        "df0 = pd.DataFrame(a, columns=['Word','Count'])\n",
        "\n",
        "a = Counter(corpus1).most_common()\n",
        "df1 = pd.DataFrame(a, columns=['Word','Count'])\n",
        "\n",
        "# Plot for Disaster and Non-Disaster\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "sns.barplot(x='Word',y='Count',data=df0.head(10), color=color[1]).set_title('Most Common Words for Non-Disasters')\n",
        "plt.xticks(rotation=45)\n",
        "plt.subplot(1,2,2)\n",
        "sns.barplot(x='Word',y='Count',data=df1.head(10), color=color[0]).set_title('Most Common Words for Disasters')\n",
        "plt.xticks(rotation=45)"
      ],
      "metadata": {
        "id": "cGkCyjmmPWiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordcloud for hashtags"
      ],
      "metadata": {
        "id": "18-W40BoPrkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(word):\n",
        "    for p in punctuation: word = word.replace(p, '')\n",
        "    return word\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def wc_hash(target):\n",
        "    hashtag = [clean(w[1:].lower()) for tweet in df[df.target == target].text for w in tweet.split() if '#' in w and w[0] == '#']\n",
        "    hashtag = ' '.join(hashtag)\n",
        "    my_cloud = WordCloud(background_color='white', stopwords=stop).generate(hashtag)\n",
        "\n",
        "    plt.subplot(1,2,target+1)\n",
        "    plt.imshow(my_cloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.figure(figsize=(15,4))\n",
        "wc_hash(0)\n",
        "plt.title('Non-Disaster')\n",
        "wc_hash(1)\n",
        "plt.title('Disaster')"
      ],
      "metadata": {
        "id": "a0yZ0gBcP1yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta-feature engineering"
      ],
      "metadata": {
        "id": "a6PFooQlQQ4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "eMgBPhy5QXB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polarity and subjectivity"
      ],
      "metadata": {
        "id": "Vp9fkLmPQZMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['polarity'] = [TextBlob(tweet).sentiment.polarity for tweet in df.text]\n",
        "df['subjectivity'] = [TextBlob(tweet).sentiment.subjectivity for tweet in df.text]"
      ],
      "metadata": {
        "id": "zEZoch5-Qcvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exclamation and question marks"
      ],
      "metadata": {
        "id": "3zQOIAmnQnrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['exclaimation_num'] = [tweet.count('!') for tweet in df.text]\n",
        "df['questionmark_num'] = [tweet.count('?') for tweet in df.text]"
      ],
      "metadata": {
        "id": "EwqnKiDpQtVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting number of hashtags and mentions"
      ],
      "metadata": {
        "id": "0LJV-9kQQq4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_url_hashtag_mention(text):\n",
        "    urls_num = len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "    word_tokens = text.split()\n",
        "    hash_num = len([word for word in word_tokens if word[0] == '#' and word.count('#') == 1]) # only appears once in front of word\n",
        "    mention_num = len([word for word in word_tokens if word[0] == '@' and word.count('@') == 1]) # only appears once in front of word\n",
        "    return urls_num, hash_num, mention_num"
      ],
      "metadata": {
        "id": "hJUmtNXJQ3QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_num, hash_num, mention_num = zip(*[count_url_hashtag_mention(tweet) for tweet in df.text])\n",
        "df = df.assign(url_num = url_num, hash_num = hash_num, mention_num = mention_num)"
      ],
      "metadata": {
        "id": "MXaW-PqYQ-gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of contractions (e.g I'm, we're, we've)"
      ],
      "metadata": {
        "id": "iMdmcKVzRLOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = [\"'t\", \"'re\", \"'s\", \"'d\", \"'ll\", \"'ve\", \"'m\"]\n",
        "df['contraction_num'] = [sum([tweet.count(cont) for cont in contractions]) for tweet in df.text]"
      ],
      "metadata": {
        "id": "1CzDDqltRRwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text data cleaning"
      ],
      "metadata": {
        "id": "6J_esfsjRXC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove or replace data"
      ],
      "metadata": {
        "id": "7aTiV2Z3UGcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Replace NaNs with 'None'\n",
        "df.keyword.fillna('None', inplace=True)"
      ],
      "metadata": {
        "id": "nNCSptK0TXyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Expand Contractions\n",
        "def decontraction(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "df.text = [decontraction(tweet) for tweet in df.text]"
      ],
      "metadata": {
        "id": "mZI47c7UTbiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Emojis\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df.text = df.text.apply(lambda x: remove_emoji(x))"
      ],
      "metadata": {
        "id": "RsZBGeBBTnCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs\n",
        "df.text = df.text.apply(lambda x: remove_url(x))"
      ],
      "metadata": {
        "id": "UrVVm6w2T550"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations except '!?'\n",
        "\n",
        "def remove_punct(text):\n",
        "    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n",
        "    table=str.maketrans('','',new_punct)\n",
        "    return text.translate(table)\n",
        "\n",
        "df.text = df.text.apply(lambda x: remove_punct(x))"
      ],
      "metadata": {
        "id": "IZC00BYuT8tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace amp\n",
        "def replace_amp(text):\n",
        "    text = re.sub(r\" amp \", \" and \", text)\n",
        "    return text\n",
        "\n",
        "df.text = df.text.apply(lambda x: replace_amp(x))"
      ],
      "metadata": {
        "id": "qlLn86SNUBjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word segmentation"
      ],
      "metadata": {
        "id": "FDYdeA5sUMZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordsegment"
      ],
      "metadata": {
        "id": "QZRNGl-9UYQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordsegment import load, segment\n",
        "load()\n",
        "\n",
        "df.text = df.text.apply(lambda x: ' '.join(segment(x)))"
      ],
      "metadata": {
        "id": "5420XiCdUOc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "7UXzNQzyWnMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemma(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(w.lower(), pos='v') for w in words])\n",
        "\n",
        "df.text = df.text.apply(lambda x: lemma(x))"
      ],
      "metadata": {
        "id": "hNplrKbfWp6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ngrams"
      ],
      "metadata": {
        "id": "lBo3cqD1Ws9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ngrams\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = word_tokenize(text)\n",
        "    return [' '.join(ngram) for ngram in list(get_data(ngrams(words, n))) if not all(w in stop for w in ngram)] # exclude if all are stopwords"
      ],
      "metadata": {
        "id": "laQrOzg3W9p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(gen):\n",
        "    try:\n",
        "        for elem in gen:\n",
        "            yield elem\n",
        "    except (RuntimeError, StopIteration):\n",
        "        return"
      ],
      "metadata": {
        "id": "UC8bm_tsZZz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams\n",
        "bigrams_disaster = df[df.target==1].text.apply(lambda x: generate_ngrams(x, 2))\n",
        "bigrams_ndisaster = df[df.target==0].text.apply(lambda x: generate_ngrams(x, 2))\n",
        "\n",
        "bigrams_d_dict = {}\n",
        "for bgs in bigrams_disaster:\n",
        "    for bg in bgs:\n",
        "        if bg in bigrams_d_dict:\n",
        "            bigrams_d_dict[bg] += 1\n",
        "        else:\n",
        "            bigrams_d_dict[bg] = 1\n",
        "\n",
        "bigrams_d_df = pd.DataFrame(bigrams_d_dict.items(), columns=['Bigrams','Count'])\n",
        "\n",
        "bigrams_nd_dict = {}\n",
        "for bgs in bigrams_ndisaster:\n",
        "    for bg in bgs:\n",
        "        if bg in bigrams_nd_dict:\n",
        "            bigrams_nd_dict[bg] += 1\n",
        "        else:\n",
        "            bigrams_nd_dict[bg] = 1\n",
        "\n",
        "bigrams_nd_df = pd.DataFrame(bigrams_nd_dict.items(), columns=['Bigrams','Count'])"
      ],
      "metadata": {
        "id": "75XvgjE9YsW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Barplots for bigrams\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "sns.barplot(x='Count',y='Bigrams',data=bigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Bigrams for Non-Disasters')\n",
        "ax = plt.gca()\n",
        "ax.set_ylabel('')\n",
        "plt.subplot(1,2,2)\n",
        "sns.barplot(x='Count',y='Bigrams',data=bigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Bigrams for Disasters')\n",
        "ax = plt.gca()\n",
        "ax.set_ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "prO4DRgJZfxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for bigrams\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "my_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_nd_dict)\n",
        "plt.imshow(my_cloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "my_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_d_dict)\n",
        "plt.imshow(my_cloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0OhKUqDZm2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigrams\n",
        "\n",
        "trigrams_disaster = df[df.target==1].text.apply(lambda x: generate_ngrams(x, 3))\n",
        "trigrams_ndisaster = df[df.target==0].text.apply(lambda x: generate_ngrams(x, 3))\n",
        "\n",
        "trigrams_d_dict = {}\n",
        "for tgs in trigrams_disaster:\n",
        "    for tg in tgs:\n",
        "        if tg in trigrams_d_dict:\n",
        "            trigrams_d_dict[tg] += 1\n",
        "        else:\n",
        "            trigrams_d_dict[tg] = 1\n",
        "\n",
        "trigrams_d_df = pd.DataFrame(trigrams_d_dict.items(), columns=['Trigrams','Count'])\n",
        "\n",
        "trigrams_nd_dict = {}\n",
        "for tgs in trigrams_ndisaster:\n",
        "    for tg in tgs:\n",
        "        if tg in trigrams_nd_dict:\n",
        "            trigrams_nd_dict[tg] += 1\n",
        "        else:\n",
        "            trigrams_nd_dict[tg] = 1\n",
        "\n",
        "trigrams_nd_df = pd.DataFrame(trigrams_nd_dict.items(), columns=['Trigrams','Count'])"
      ],
      "metadata": {
        "id": "GZnpVfBNZtiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Barplots for trigrams\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "sns.barplot(x='Count',y='Trigrams',data=trigrams_nd_df.sort_values('Count', ascending=False).head(40), color=color[0]).set_title('Most Common Trigrams for Non-Disasters')\n",
        "ax = plt.gca()\n",
        "ax.set_ylabel('')\n",
        "plt.subplot(1,2,2)\n",
        "sns.barplot(x='Count',y='Trigrams',data=trigrams_d_df.sort_values('Count', ascending=False).head(40), color=color[1]).set_title('Most Common Trigrams for Disasters')\n",
        "ax = plt.gca()\n",
        "ax.set_ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vn3hjMXIZykI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove stopwords"
      ],
      "metadata": {
        "id": "o2AILHEoZ1d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove Stopwords\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return ' '.join([w.lower() for w in word_tokens if not w.lower() in stop])\n",
        "\n",
        "#tweets_tmp = tweets.copy()\n",
        "df['text_nostopwords'] = df.text.apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "LxOyrBpFZ3cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot word cloud for most common words after cleaning"
      ],
      "metadata": {
        "id": "M50w0YOwaXWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "mask = np.array(Image.open('twitter.png'))\n",
        "reverse = mask[...,::-1,:]"
      ],
      "metadata": {
        "id": "W5PeieUcabi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wc_words(target, mask=mask):\n",
        "    words = [word.lower() for tweet in df[df.target == target].text_nostopwords for word in tweet.split()]\n",
        "    words = list(filter(lambda w: w != 'like', words))\n",
        "    words = list(filter(lambda w: w != 'new', words))\n",
        "    words = list(filter(lambda w: w != 'people', words))\n",
        "    dict = {}\n",
        "    for w in words:\n",
        "        if w in dict:\n",
        "            dict[w] += 1\n",
        "        else:\n",
        "            dict[w] = 1\n",
        "    # plot using frequencies\n",
        "    my_cloud = WordCloud(background_color='white', stopwords=stop, mask=mask, random_state=0).generate_from_frequencies(dict)\n",
        "\n",
        "    plt.subplot(1,2,target+1)\n",
        "    plt.imshow(my_cloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "wc_words(0)\n",
        "plt.title('Non-Disaster')\n",
        "wc_words(1, reverse)\n",
        "plt.title('Disaster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWYSkyp0aqF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 200\n",
        "for t in df['text'].sample(n=20, random_state=0):\n",
        "    print(t)\n",
        "pd.reset_option('max_colwidth')"
      ],
      "metadata": {
        "id": "9qgZ55hccD1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('max_colwidth')\n",
        "df.drop('text_nostopwords', axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EWjTDqqjcIx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "T-A7W047Dyz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizing dataframe setup"
      ],
      "metadata": {
        "id": "D1I3opZ5nAhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "AT2-i2m8D3fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['location'], axis=1)"
      ],
      "metadata": {
        "id": "QE-rpJLcnDxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "bVBY7VIvoIEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['input'] = 'TEXT: ' + df.text_nostopwords + '; KEYWORD: ' + df.keyword"
      ],
      "metadata": {
        "id": "eSNk8kfxnzMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset,DatasetDict\n",
        "\n",
        "ds = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "xwh7dqTtBuQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nm = 'bert-base-uncased'"
      ],
      "metadata": {
        "id": "Gbpm1KVCEQ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hEv0Edv2EavZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "Bhy2sds9E1Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
        "tokz = AutoTokenizer.from_pretrained(model_nm)"
      ],
      "metadata": {
        "id": "f89y5spGEXV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tok_func(x): return tokz(x[\"input\"])"
      ],
      "metadata": {
        "id": "VmjgY0TJEZG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ds = ds.map(tok_func, batched=True)"
      ],
      "metadata": {
        "id": "sqlBnQluGQeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = tok_ds[0]\n",
        "row['input'], row['input_ids']"
      ],
      "metadata": {
        "id": "PhC4l7DZGSZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokz.tokenize(ds[\"input\"][0])"
      ],
      "metadata": {
        "id": "1mPbD_HBGVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokz.vocab['earthquake']"
      ],
      "metadata": {
        "id": "8W9kKeiPGgph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ds = tok_ds.rename_columns({'target':'labels'})"
      ],
      "metadata": {
        "id": "OgrOPK2YHBzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ds"
      ],
      "metadata": {
        "id": "KXdctMe7Hop4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test and validation sets"
      ],
      "metadata": {
        "id": "y1NJrKRXH5y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = pd.read_csv('./test.csv')\n",
        "eval_df.describe()"
      ],
      "metadata": {
        "id": "F2aOyTl9HpYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation set"
      ],
      "metadata": {
        "id": "GfTQ7SxlI6hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dds = tok_ds.train_test_split(0.25, seed=42)\n",
        "dds"
      ],
      "metadata": {
        "id": "r4-658prIMAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test set"
      ],
      "metadata": {
        "id": "C1lUGczqJO6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df['input'] = 'TEXT: ' + df.text + '; KEYWORD: ' + df.keyword\n",
        "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
      ],
      "metadata": {
        "id": "TGZinwbQI0t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measure coef"
      ],
      "metadata": {
        "id": "AocbKQHsKmpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "HLfIALplLR2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_val(eval_pred): return {'f1_val': f1_score(*eval_pred)}"
      ],
      "metadata": {
        "id": "NNat93eiJ47N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "UTunJz1zL5cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training our model"
      ],
      "metadata": {
        "id": "DGGhrChKL67w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "-0SyaAmbMK2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments,Trainer"
      ],
      "metadata": {
        "id": "ljDNI9DeL6VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 128\n",
        "epochs = 4"
      ],
      "metadata": {
        "id": "vLksIO2DL9e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 8e-5"
      ],
      "metadata": {
        "id": "cbcHdyR1L_iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "oYUbUaFx3PbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
      ],
      "metadata": {
        "id": "hwgyB6hRMD1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
        "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                  tokenizer=tokz, compute_metrics=f1_val)"
      ],
      "metadata": {
        "id": "bW7x3Mo3MHqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train();"
      ],
      "metadata": {
        "id": "YQMpXf0JNPcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "Kf06eJKvPYVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rKUkFRIPqaWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}